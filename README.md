# ğŸ§  Image Caption Generator using ResNet50 and LSTM

This project is an end-to-end deep learning solution that automatically generates natural language descriptions for images. It integrates a powerful feature extractor (DenseNet201) with a language modeling component (LSTM), trained on the **Flickr30k** dataset. The final model is deployed using a **Flask** web app for real-time image captioning.

---

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ static/                   # Static assets (CSS, JS, images, etc.)
â”œâ”€â”€ templates/                # HTML templates for the web app
â”œâ”€â”€ .gitignore                # Git ignore rules
â”œâ”€â”€ 01-data-preparation-8k.ipynb    # Data preprocessing notebook
â”œâ”€â”€ 02-model-training-8k.ipynb      # Model training notebook
â”œâ”€â”€ 03-model-evaluation-8k.ipynb    # Model evaluation notebook
â”œâ”€â”€ app.py                    # Main FastAPI/Flask app script
â”œâ”€â”€ caption_model.weights.h5  # Trained model weights (HDF5 format)
â”œâ”€â”€ captions.pkl              # Preprocessed captions data
â”œâ”€â”€ config.pkl                # Configuration parameters
â”œâ”€â”€ features.pkl              # Extracted image features
â”œâ”€â”€ generate_captions.py      # Script to generate captions from images
â”œâ”€â”€ history.pkl               # Training history (loss/accuracy)
â”œâ”€â”€ model.keras               # Saved Keras model
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ test.pkl                  # Test dataset
â””â”€â”€ tokenizer.pkl             # Tokenizer for text preprocessing

```

---

## ğŸ§  Model Architecture

### ğŸ”¹ Encoder (Feature Extractor)
- Pretrained **ResNet50** from Keras Applications
- Global Average Pooling applied to last convolutional layer output

### ğŸ”¹ Decoder (Language Generator)
- Embedding Layer initialized randomly
- LSTM Layer with 256 hidden units
- Dense Layer with ReLU followed by Softmax over vocabulary

The model is trained to predict the next word given the image embedding and previously generated words.

---

## ğŸ“ˆ Evaluation Metrics
Model performance is evaluated using BLEU-n metrics over a held-out validation set:
```
BLEU-1: 0.65
BLEU-2: 0.40
```
> Scores were calculated using 5 reference captions per image with NLTKâ€™s BLEU implementation and smoothing function.

---

## ğŸŒ Deployment with Flask

A simple web UI is included for testing the model by uploading an image.

### Features
- Upload any image from your device
- Automatically view the predicted caption

### Run the App Locally
```bash
# Step 1: Set up environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Step 2: Install dependencies
pip install -r requirements.txt

# Step 3: Launch app
python app.py
```

Then navigate to `http://127.0.0.1:5000` in your browser.

---

## ğŸ“¦ Dependencies
- `tensorflow`
- `flask`
- `numpy`, `pandas`, `matplotlib`, `Pillow`
- `nltk`, `tqdm`

To install all dependencies:
```bash
pip install -r requirements.txt
```

---

## ğŸ—ƒï¸ Dataset
- [Flickr30k Dataset](https://www.kaggle.com/datasets/eeshawn/flickr30k)

---

## ğŸ‘¤ Author
**Arindam Phatowali**  
B.Tech + M.Tech (Mathematics & Data Science), MANIT Bhopal

---

## ğŸ“¬ Contact & Links
ğŸ“§ Email: arindamphatowali@gmail.com  
ğŸ GitHub: [github.com/arinp95](https://github.com/arinp95)

---

## â­ Like this project?
Please consider starring â­ the repository to show your support and help others discover it!


# ğŸ–¼ï¸ Image Captioning with ResNet50 + LSTM

This project explores how **deep learning** can connect **computer vision** and **natural language processing** to describe images in natural language. The goal is simple but powerful: **given an image, automatically generate a meaningful caption**.

To achieve this, the project combines:
- **ResNet50 (CNN)** â†’ to extract rich visual features from an image.
- **LSTM (RNN)** â†’ to generate a descriptive sentence word by word.
- **Flickr8k dataset** â†’ 8,000 images with 5 captions each, providing a diverse training ground.

On top of the model, we built a **FastAPI-powered web app**, where users can upload images and receive captions instantly.

---

## ğŸ“– Problem Statement

Humans can easily look at an image and describe it: *â€œa dog running across a fieldâ€*. For a computer, this is challenging because it requires:
1. **Understanding the visual content** (objects, actions, relationships).
2. **Translating that understanding into language** (grammar, word order, fluency).

This project aims to bridge that gap by training a model that learns **visualâ€“linguistic mappings**.

---

## ğŸ” Approach

1. **Feature Extraction (Vision / Encoder)**
   - Uses **ResNet50** (a pretrained CNN) to process input images.
   - The network outputs a **2048-dimensional feature vector** summarizing the image.

2. **Caption Generation (Language / Decoder)**
   - An **LSTM network** takes the encoded features and sequentially predicts words.
   - Captions start with a `<start>` token and end with an `<end>` token.
   - During training, the model learns from human-written captions.

3. **Training Pipeline**
   - Captions are cleaned and tokenized.
   - Maximum caption length and vocabulary size are saved in `config.pkl`.
   - Extracted features (`features.pkl`) speed up training.
   - Training history (loss, accuracy) is logged in `history.pkl`.

4. **Evaluation**
   - Quality measured using **BLEU scores**, which compare generated captions with ground truth references.
   - BLEU-1 captures word overlap; BLEU-4 considers longer phrase overlaps.

5. **Deployment (Web App)**
   - A **FastAPI server** (`app.py`) powers the web interface.
   - Upload an image â†’ model predicts caption â†’ result displayed in the browser.

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ static/                   # Static assets (CSS, JS, uploaded images)
â”œâ”€â”€ templates/                # HTML templates for the web app
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”œâ”€â”€ 01-data-preparation-8k.ipynb    # Preprocessing images & captions
â”œâ”€â”€ 02-model-training-8k.ipynb      # Model architecture & training loop
â”œâ”€â”€ 03-model-evaluation-8k.ipynb    # Caption evaluation & BLEU scoring
â”œâ”€â”€ app.py                     # FastAPI web server
â”œâ”€â”€ caption_model.weights.h5   # Trained model weights
â”œâ”€â”€ captions.pkl               # Preprocessed captions
â”œâ”€â”€ config.pkl                 # Config (max caption length, vocab size, etc.)
â”œâ”€â”€ features.pkl               # Precomputed image features
â”œâ”€â”€ generate_captions.py       # Core CaptionGenerator class (ResNet50 + LSTM)
â”œâ”€â”€ history.pkl                # Training history (loss/accuracy curves)
â”œâ”€â”€ model.keras                # Full Keras model (saved format)
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ test.pkl                   # Test dataset
â””â”€â”€ tokenizer.pkl              # Tokenizer (word â†” index mapping)
```

---

## âš™ï¸ Installation & Setup

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/image-captioning.git
cd image-captioning
```

### 2. Create a Virtual Environment
```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate    # Windows
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Download Data & Pretrained Files
- **Dataset**: [Flickr8k](https://github.com/jbrownlee/Datasets/releases/tag/Flickr8k)
- Place the following in the project root:
  - `model.keras` or `caption_model.weights.h5`
  - `tokenizer.pkl`
  - `config.pkl`

---

## â–¶ï¸ Running the Application

Start the FastAPI server:
```bash
uvicorn app:app --reload
```

Open your browser and visit:
```
http://127.0.0.1:8000
```

1. Upload an image.  
2. The model extracts features with **ResNet50**.  
3. The **LSTM decoder** generates a caption.  
4. The caption is displayed alongside the uploaded image.  

---

## ğŸ§  Model Architecture

**Encoder (ResNet50):**
- Pretrained on ImageNet.
- Outputs 2048-d feature vector for each image.

**Decoder (LSTM):**
- Input: Encoded image + partial caption sequence.
- Embedding layer for word vectors.
- LSTM generates next word probabilities.
- Dense layer maps outputs to vocabulary size.

**Training Objective:**
- **Categorical Cross-Entropy Loss** between predicted and true next words.

---

## ğŸ“Š Example Results

| Input Image | Generated Caption |
|-------------|------------------|
| ![dog](https://i.imgur.com/jxWqIBf.jpg) | "a dog running across a grassy field" |
| ![kids](https://i.imgur.com/62Qp6uh.jpg) | "two children playing in the park" |
| ![beach](https://i.imgur.com/E8mQl2O.jpg) | "people walking along the beach" |

---

## ğŸ”® Future Improvements

- Use **beam search** instead of greedy decoding for more natural captions.
- Replace LSTM with **Transformers** (e.g., attention-based decoders).
- Train on larger datasets like **MS-COCO** for richer vocabulary.
- Deploy on **Docker, Hugging Face Spaces, or AWS Lambda** for easier access.

---

## ğŸ™Œ Acknowledgements

- Dataset: [Flickr8k](https://github.com/jbrownlee/Datasets/releases/tag/Flickr8k)
- Libraries: [TensorFlow/Keras](https://www.tensorflow.org/), [FastAPI](https://fastapi.tiangolo.com/)
- Pretrained ResNet50 from [Keras Applications](https://keras.io/api/applications/).

---

## ğŸ“œ License

This project is released under the **MIT License**.  
You are free to use, modify, and distribute it with attribution.

